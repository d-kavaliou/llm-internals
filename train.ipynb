{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1292bd51-379e-467a-902e-5f3480af96f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2c327690-bc3e-460b-93db-9ea957d97ffa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import tiktoken\n",
    "\n",
    "from torch.nn import functional as F\n",
    "from torchviz import make_dot\n",
    "\n",
    "from gpt.model import Gpt, detect_device\n",
    "from gpt.loader import DataLoaderLite\n",
    "from gpt.config import GPTConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ee56f7a3-3fcc-42c9-9b92-45d81d11102a",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = detect_device()\n",
    "enc = tiktoken.get_encoding('gpt2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "82364f54-e98b-4f34-9a29-d2d618eb23b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(1337)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(1337)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "08f62e6f-e873-487b-92fd-ecde1068e354",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded 338024 tokens\n",
      "1 epoch = 2640 batches\n"
     ]
    }
   ],
   "source": [
    "train_loader = DataLoaderLite(B=4, T=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6e5eb47c-173a-47a9-9eca-64db0f0ee999",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = GPTConfig()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7927a83d-1c54-4106-9932-73a31b486e08",
   "metadata": {},
   "outputs": [],
   "source": [
    "config.n_layer = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "fd72b0a0-b4b3-4ad7-921f-5f147210dc4b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Gpt(\n",
       "  (transformer): ModuleDict(\n",
       "    (wte): Embedding(50257, 768)\n",
       "    (wpe): Embedding(1024, 768)\n",
       "    (h): ModuleList(\n",
       "      (0): Block(\n",
       "        (att_norm): LayerNorm()\n",
       "        (attention): Attention(\n",
       "          (c_attn): Linear(in_features=768, out_features=2304, bias=True)\n",
       "          (c_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (layer_norm): LayerNorm()\n",
       "        (mlp): MLP(\n",
       "          (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (gelu): GELU(approximate='tanh')\n",
       "          (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Gpt(config)\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27b531e0-a473-44e4-ac20-7e74f3eaf1f5",
   "metadata": {},
   "source": [
    "### Debugging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a2fbf3c2-1c6c-4265-9e84-3e93e0b558dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = train_loader.next_batch()\n",
    "x, y = x.to(device), y.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "1e0d9f77-a959-4f3a-9793-4197e27deb29",
   "metadata": {},
   "outputs": [],
   "source": [
    "logits, loss = model(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "367ae40b-5975-49b5-b208-b9bb989605ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'computational_graph.png'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dot = make_dot(loss, params=dict(model.named_parameters()))\n",
    "dot.render('computational_graph', format='png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "89a5a1da-878e-4259-955b-f709f137f891",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'computational_graph_with_grads.png'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss.backward(retain_graph=True)  # retain_graph=True to allow multiple backwards\n",
    "dot = make_dot(loss, \n",
    "               params=dict(list(model.named_parameters()) + [('input', x)]),\n",
    "               show_attrs=True,  # Show tensor sizes and other attributes\n",
    "               show_saved=True)  # Show saved tensors for backward\n",
    "dot.render('computational_graph_with_grads', format='png')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ca20338-cccc-459a-904d-087e53f2a9ff",
   "metadata": {},
   "source": [
    "## Train loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9fc3aca3-8684-476e-a555-fc4e6bdfec30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0, loss: 10.880291938781738\n"
     ]
    }
   ],
   "source": [
    "optimizer = torch.optim.AdamW(model.parameters(), lr=3e-4)\n",
    "for i in range(50):\n",
    "    x, y = train_loader.next_batch()\n",
    "    x, y = x.to(device), y.to(device)\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    logits, loss = model(x, y)\n",
    "    \n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    print(f\"step {i}, loss: {loss.item()}\")\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0508d380-8788-4d9f-9570-493ffbda0eb5",
   "metadata": {},
   "source": [
    "## Model eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cab3acef-f3d9-43ec-9121-15aaaae96ee2",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "num_return_sequences = 5\n",
    "max_length = 30\n",
    "tokens = enc.encode(\"Hello, I'm a language model,\")\n",
    "tokens = torch.tensor(tokens, dtype=torch.long) # (8,)\n",
    "tokens = tokens.unsqueeze(0).repeat(num_return_sequences, 1) # (5, 8)\n",
    "x = tokens.to(device)\n",
    "\n",
    "# generate! right now x is (B, T) where B = 5, T = 8\n",
    "# set the seed to 42\n",
    "torch.manual_seed(42)\n",
    "torch.cuda.manual_seed(42)\n",
    "while x.size(1) < max_length:\n",
    "    # forward the model to get the logits\n",
    "    with torch.no_grad():\n",
    "        logits, loss = model(x) # (B, T, vocab_size)\n",
    "        # take the logits at the last position\n",
    "        logits = logits[:, -1, :] # (B, vocab_size)\n",
    "        # get the probabilities\n",
    "        probs = F.softmax(logits, dim=-1)\n",
    "        # do top-k sampling of 50 (huggingface pipeline default)\n",
    "        # topk_probs here becomes (5, 50), topk_indices is (5, 50)\n",
    "        topk_probs, topk_indices = torch.topk(probs, 50, dim=-1)\n",
    "        # select a token from the top-k probabilities\n",
    "        # note: multinomial does not demand the input to sum to 1\n",
    "        ix = torch.multinomial(topk_probs, 1) # (B, 1)\n",
    "        # gather the corresponding indices\n",
    "        xcol = torch.gather(topk_indices, -1, ix) # (B, 1)\n",
    "        # append to the sequence\n",
    "        x = torch.cat((x, xcol), dim=1)\n",
    "\n",
    "# print the generated text\n",
    "for i in range(num_return_sequences):\n",
    "    tokens = x[i, :max_length].tolist()\n",
    "    decoded = enc.decode(tokens)\n",
    "    print(\">\", decoded)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
